우선 웹서버와 REST API 서버는 이렇게 분리된다.
![[Pasted image 20221027151700.png]]
그래서 아래 순서로 화면을 뿌림
1. client - web
	1. client에서 요청을 주면 
	2. 웹서버에서 HTML, js 파일 제공
2. client - was : js를 받은 클라이언트가 API 서버를 호출

# 서빙 패턴
[git page](https://mercari.github.io/ml-system-design-pattern/README_ko.html)
## web single pattern
웹서버 하나에서 프로세스, 전처리, 훈련 , REST 인터페이스 까지 전담
![[Pasted image 20221027152046.png]]
모델은 [[디자인패턴#Model-in-image pattern]]  [[디자인패턴#Model-load pattern]]으로 운영

## Synchronous pattern
웹서버 요청을 동기화 처리. 즉 모델 예측결과 받기전까지 프로세스 블락
![[Pasted image 20221027152153.png]]

## Asynchronous pattern
클라이언트 - 큐or캐시 - 예측서버. 예측 요청과 검색을 분리
![[Pasted image 20221027152618.png]]
## batch pattern
예측결과를 실시간, 준실시간으로 필요로하지 않는 경우
대량의 데이터에 대한 예측이 필요한 경우

## Process-prediction pattern
데이터 로드, 전처리, 예측을 각각 분리할 필요가 있는 경우
아래처럼 서버를 선형적으로 배치
![[Pasted image 20221027153004.png]]
아래처럼 프록시 서버를 두고 단계별로 각각의 컨테이너에 요청하기도 한다
![[Pasted image 20221027152915.png]]

## Microservice vertical pattern
여러 모델을 순서대로 예측해야하는 경우. 예측끼리 의존관계(순서 종속성)가 있는 경우
아래처럼 프록시 서버에서 예측 순서를 정의하거나
![[Pasted image 20221027153632.png]]
아래 처럼 프록시 서버에 예측순서 정의, 프록시 서버와 모델 서버에서 데이터 추가 검색 가능
![[Pasted image 20221027153900.png]]
## Microservice hrizontarl pattern
여러 예측 결과를 병렬실행 & 통합해야할 필요가 있는 경우
요청 동기화로 구성할 경우 모든 예측 결과를 집계해 반환
![[Pasted image 20221027154219.png]]
비동기화로 구성할 경우 큐 필요. 한 예측만 나와도 다음 작업 가능
![[Pasted image 20221027154234.png]]
## Data cache pattern
동일 요청이 반복될 때 속도 높히고 싶은 경우
입력 데이터를 캐싱.
요청 -> DW에 요청 데이터 저장 & 동시에 캐시 확인 -> 캐시에 예측결과 없으면 예측 요청
![[Pasted image 20221027154638.png]]
미리 전체 데이터에 대한 캐싱을 해 놓는 구성있음 -> 속도면에 유리
미리 DW 데이터로 캐싱 -> 요청을 캐시서버에서 검색 -> 없으면 예측 서버에 예측 요청
![[Pasted image 20221027154647.png]]

## Prediction circuit break pattern
예측 요청이 가자기 증가하고, 스케일아웃 하기 힘든 경우에 서버 한계이상의 요청은 취소

## Multiple stage prediction pattern
정형 데이터를 처리하는 가벼운 모델을 앞에두고
비정형 데이터를 처리하는 무거운 모델을 뒤에둬서
일단 가벼운 모델 결과를 던져주고, 무거운 모델의 예측결과를 비동기화하여 요청
![[Pasted image 20221027155326.png]]

## Anti-pattern : 이렇게는 하지마라
1. All in one pattern : 다일 서버(그룹) 안에 여러 예측모델을 실행하는 경우
	- 왜냐면
		1. 라이브러리 선택에 제한
		2. 장애 발생 시 전체 로그를 확인해야함 = 복잡성 증가
2. Online bigsize pattern : 실시간 시스템에 예측시간 오래걸리는 큰 모델 배치하는 경우

# 학습 패턴
## Batch training pattern
모델 학습을 주기적으로 실행
워크플로우 : 데이터 수집 -> 전처리 -> 학습 -> 평가 -> 예측서버에 모델 저장 -> 평가 기록
![[Pasted image 20221028092834.png]]
## Pipeline training pattern
배치 학습 패턴의 응용패턴
학습 과정을 선형 의존관계가 있는 워크플로우로 구성
![[Pasted image 20221028093241.png]]
## Parameter and architecture search pattern
다양한 모델의 하이퍼 파라미터와 아키텍쳐를 자동 탐색하고싶을 경우
머신러닝 모델 자동 튜닝
모델 개발 시 필요한 수동 작업 최소화
![[Pasted image 20221028093445.png]]

# 모델 운영 패턴
모델을 설치하는 방법에 따라
1. model in image pattern
2. model load pattern
다수의 모델을 운영해야 하는 경우
- 마이크로 서비스 아키텍처

## Model-in-image pattern
서버, 컨테이너 이미지에 모델을 포함하여 관리
서버 컨테이너 이미지, 모델 학습, 이미지 빌드까지 하나의 워크플로우로 정의
모델 버전관리 필요 없음 : 이미지 버전과 모델 버전이 동일
![[Pasted image 20221028093706.png]]

## Model-load pattern
서버 이미지와 모델 파일을 분리
서버는 하나, 모델은 여러개(버전, 알고리즘...)
배포 : 서버 배포 -> 모델 이미지 파일다운로드
모델 라이브러리 버전 관리 주의
![[Pasted image 20221028094105.png]]

## Data model versioning pattern
네이밍 룰
![[Pasted image 20221028095723.png]]
단계 별 모델 관리 전략
![[Pasted image 20221028095747.png]]

