우선 웹서버와 REST API 서버는 이렇게 분리된다.
![[Pasted image 20221027151700.png]]
그래서 아래 순서로 화면을 뿌림
1. client - web
	1. client에서 요청을 주면 
	2. 웹서버에서 HTML, js 파일 제공
2. client - was : js를 받은 클라이언트가 API 서버를 호출

# 서빙 패턴
[git page](https://mercari.github.io/ml-system-design-pattern/README_ko.html)
## web single pattern
웹서버 하나에서 프로세스, 전처리, 훈련 , REST 인터페이스 까지 전담
![[Pasted image 20221027152046.png]]
모델은 [[디자인패턴#Model-in-image pattern]]  [[디자인패턴#Model-load pattern]]으로 운영

## Synchronous pattern
웹서버 요청을 동기화 처리. 즉 모델 예측결과 받기전까지 프로세스 블락
![[Pasted image 20221027152153.png]]

## Asynchronous pattern
클라이언트 - 큐or캐시 - 예측서버. 예측 요청과 검색을 분리
![[Pasted image 20221027152618.png]]
## batch pattern
예측결과를 실시간, 준실시간으로 필요로하지 않는 경우
대량의 데이터에 대한 예측이 필요한 경우

## Process-prediction pattern
데이터 로드, 전처리, 예측을 각각 분리할 필요가 있는 경우
아래처럼 서버를 선형적으로 배치
![[Pasted image 20221027153004.png]]
아래처럼 프록시 서버를 두고 단계별로 각각의 컨테이너에 요청하기도 한다
![[Pasted image 20221027152915.png]]

## Microservice vertical pattern
여러 모델을 순서대로 예측해야하는 경우. 예측끼리 의존관계(순서 종속성)가 있는 경우
아래처럼 프록시 서버에서 예측 순서를 정의하거나
![[Pasted image 20221027153632.png]]
아래 처럼 프록시 서버에 예측순서 정의, 프록시 서버와 모델 서버에서 데이터 추가 검색 가능
![[Pasted image 20221027153900.png]]
## Microservice hrizontarl pattern
여러 예측 결과를 병렬실행 & 통합해야할 필요가 있는 경우
요청 동기화로 구성할 경우 모든 예측 결과를 집계해 반환
![[Pasted image 20221027154219.png]]
비동기화로 구성할 경우 큐 필요. 한 예측만 나와도 다음 작업 가능
![[Pasted image 20221027154234.png]]
## Data cache pattern
동일 요청이 반복될 때 속도 높히고 싶은 경우
입력 데이터를 캐싱.
요청 -> DW에 요청 데이터 저장 & 동시에 캐시 확인 -> 캐시에 예측결과 없으면 예측 요청
![[Pasted image 20221027154638.png]]
미리 전체 데이터에 대한 캐싱을 해 놓는 구성있음 -> 속도면에 유리
미리 DW 데이터로 캐싱 -> 요청을 캐시서버에서 검색 -> 없으면 예측 서버에 예측 요청
![[Pasted image 20221027154647.png]]

## Prediction circuit break pattern
예측 요청이 가자기 증가하고, 스케일아웃 하기 힘든 경우에 서버 한계이상의 요청은 취소

## Multiple stage prediction pattern
정형 데이터를 처리하는 가벼운 모델을 앞에두고
비정형 데이터를 처리하는 무거운 모델을 뒤에둬서
일단 가벼운 모델 결과를 던져주고, 무거운 모델의 예측결과를 비동기화하여 요청
![[Pasted image 20221027155326.png]]


# 모델 운영 패턴
## Model-in-image pattern
## Model-load pattern
